{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b38c790c-cdce-493b-9685-e25482a937fd",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONFIGURA√á√ÉO DO AMBIENTE ===\n",
      "‚úì Python: 3.10.0\n",
      "‚úì Execut√°vel: C:\\Users\\Gabriel\\Documents\\ml-data-studying-projects\\venv_spark\\Scripts\\python.exe\n",
      "============================================================\n",
      "\n",
      "=== IMPORTANDO BIBLIOTECAS ===\n",
      "‚úì Bibliotecas importadas!\n",
      "\n",
      "=== CRIANDO SPARK SESSION ===\n",
      "‚úì Spark Session criada!\n",
      "‚úì Vers√£o: 4.0.1\n",
      "‚úì App Name: ETL_COVID_to_SQL\n",
      "\n",
      "=== FASE 1: EXTRACT (Extra√ß√£o) ===\n",
      "üì• Baixando dados de: https://raw.githubusercontent.com/wcota/covid19br/master/cases-brazil-cities.csv\n",
      "‚úì Dados baixados: ../data/raw/covid19_brazil_raw.csv\n",
      "‚úì Tamanho: 0.54 MB\n",
      "\n",
      "=== LENDO CSV COM SPARK ===\n",
      "‚úì CSV lido com sucesso!\n",
      "‚úì Total de registros: 5,596\n",
      "‚úì Total de colunas: 14\n",
      "\n",
      "üìä Primeiras linhas dos dados brutos:\n",
      "+-------+-----+----------------------+-------+------+----------+---------------------------+-------------------------------+--------------------+-------+----------+--------+---------+--------------+\n",
      "|country|state|city                  |ibgeID |deaths|totalCases|deaths_per_100k_inhabitants|totalCases_per_100k_inhabitants|deaths_by_totalCases|_source|date      |newCases|newDeaths|last_info_date|\n",
      "+-------+-----+----------------------+-------+------+----------+---------------------------+-------------------------------+--------------------+-------+----------+--------+---------+--------------+\n",
      "|Brazil |GO   |Abadia de Goi√°s/GO    |5200050|50    |4074      |545.97074                  |44485.69557                    |0.01227             |MS     |2023-03-18|5       |0        |2023-03-18    |\n",
      "|Brazil |MG   |Abadia dos Dourados/MG|3100104|21    |1943      |299.0601                   |27670.17944                    |0.01081             |MS     |2023-03-18|0       |0        |2023-03-18    |\n",
      "|Brazil |GO   |Abadi√¢nia/GO          |5200100|55    |2218      |263.4983                   |10626.16778                    |0.0248              |MS     |2023-03-18|-1      |0        |2023-03-18    |\n",
      "|Brazil |PA   |Abaetetuba/PA         |1500107|244   |12009     |152.08272                  |7485.08779                     |0.02032             |MS     |2023-03-18|13      |0        |2023-03-18    |\n",
      "|Brazil |MG   |Abaet√©/MG             |3100203|51    |4279      |219.23226                  |18394.01625                    |0.01192             |MS     |2023-03-18|0       |0        |2023-03-18    |\n",
      "+-------+-----+----------------------+-------+------+----------+---------------------------+-------------------------------+--------------------+-------+----------+--------+---------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "=== AN√ÅLISE EXPLORAT√ìRIA ===\n",
      "\n",
      "1. Schema dos dados:\n",
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- ibgeID: integer (nullable = true)\n",
      " |-- deaths: integer (nullable = true)\n",
      " |-- totalCases: integer (nullable = true)\n",
      " |-- deaths_per_100k_inhabitants: double (nullable = true)\n",
      " |-- totalCases_per_100k_inhabitants: double (nullable = true)\n",
      " |-- deaths_by_totalCases: double (nullable = true)\n",
      " |-- _source: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- newCases: integer (nullable = true)\n",
      " |-- newDeaths: integer (nullable = true)\n",
      " |-- last_info_date: date (nullable = true)\n",
      "\n",
      "\n",
      "2. Colunas dispon√≠veis:\n",
      "   1. country\n",
      "   2. state\n",
      "   3. city\n",
      "   4. ibgeID\n",
      "   5. deaths\n",
      "   6. totalCases\n",
      "   7. deaths_per_100k_inhabitants\n",
      "   8. totalCases_per_100k_inhabitants\n",
      "   9. deaths_by_totalCases\n",
      "   10. _source\n",
      "   11. date\n",
      "   12. newCases\n",
      "   13. newDeaths\n",
      "   14. last_info_date\n",
      "\n",
      "3. Estat√≠sticas descritivas:\n",
      "+-------+-------+-----+------------------+------------------+------------------+------------------+---------------------------+-------------------------------+--------------------+-------+-----------------+------------------+\n",
      "|summary|country|state|              city|            ibgeID|            deaths|        totalCases|deaths_per_100k_inhabitants|totalCases_per_100k_inhabitants|deaths_by_totalCases|_source|         newCases|         newDeaths|\n",
      "+-------+-------+-----+------------------+------------------+------------------+------------------+---------------------------+-------------------------------+--------------------+-------+-----------------+------------------+\n",
      "|  count|   5596| 5596|              5596|              5596|              5596|              5596|                       5596|                           5596|                5596|   5596|             5596|              5596|\n",
      "|   mean|   NULL| NULL|              NULL| 3238474.146890636|124.96604717655468| 6627.147962830593|         264.41535558970577|             18658.703739522953|0.016964592566118687|   NULL|              0.0|               0.0|\n",
      "| stddev|   NULL| NULL|              NULL|1007225.7318231501|  916.348086215888|33093.529677000035|         239.98985344333795|             11756.863054587982|0.012580441630968863|   NULL|352.5162297330446|2.1629754768178593|\n",
      "|    min| Brazil|   AC|Abadia de Goi√°s/GO|                11|              -258|            -37026|                   -0.69505|                      -197.4378|                 0.0|     GI|           -17239|              -121|\n",
      "|    max| Brazil|   TO|           √ìleo/SP|           5300108|             44671|           1316242|                14039.97865|                   337540.77941|              0.3125|     MS|            10164|                52|\n",
      "+-------+-------+-----+------------------+------------------+------------------+------------------+---------------------------+-------------------------------+--------------------+-------+-----------------+------------------+\n",
      "\n",
      "\n",
      "4. Contagem de valores nulos por coluna:\n",
      "+-------+-----+----+------+------+----------+---------------------------+-------------------------------+--------------------+-------+----+--------+---------+--------------+\n",
      "|country|state|city|ibgeID|deaths|totalCases|deaths_per_100k_inhabitants|totalCases_per_100k_inhabitants|deaths_by_totalCases|_source|date|newCases|newDeaths|last_info_date|\n",
      "+-------+-----+----+------+------+----------+---------------------------+-------------------------------+--------------------+-------+----+--------+---------+--------------+\n",
      "|      0|    0|   0|     0|     0|         0|                          0|                              0|                   0|      0|   0|       0|        0|             0|\n",
      "+-------+-----+----+------+------+----------+---------------------------+-------------------------------+--------------------+-------+----+--------+---------+--------------+\n",
      "\n",
      "\n",
      "=== FASE 2: TRANSFORM (Transforma√ß√£o) ===\n",
      "üßπ Iniciando limpeza dos dados...\n",
      "‚úì Colunas selecionadas: ['date', 'state', 'city', 'totalCases', 'newCases', 'deaths', 'newDeaths']\n",
      "\n",
      "üîç Removendo valores nulos...\n",
      "‚úì Registros removidos: 0\n",
      "‚úì Registros restantes: 5,596\n",
      "\n",
      "üîç Removendo duplicatas...\n",
      "‚úì Duplicatas removidas: 0\n",
      "\n",
      "üîÑ Convertendo tipos de dados...\n",
      "\n",
      "‚ûï Adicionando colunas calculadas...\n",
      "\n",
      "üî§ Renomeando colunas...\n",
      "‚úì Transforma√ß√£o conclu√≠da!\n",
      "\n",
      "=== DADOS LIMPOS ===\n",
      "\n",
      "üìä Schema final:\n",
      "root\n",
      " |-- data: date (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      " |-- cidade: string (nullable = true)\n",
      " |-- casos_totais: integer (nullable = true)\n",
      " |-- casos_novos: integer (nullable = true)\n",
      " |-- obitos_totais: integer (nullable = true)\n",
      " |-- obitos_novos: integer (nullable = true)\n",
      " |-- ano: integer (nullable = true)\n",
      " |-- mes: integer (nullable = true)\n",
      " |-- data_carga: timestamp (nullable = false)\n",
      "\n",
      "\n",
      "üìä Amostra dos dados limpos:\n",
      "+----------+------+--------------------+------------+-----------+-------------+------------+----+---+--------------------+\n",
      "|      data|estado|              cidade|casos_totais|casos_novos|obitos_totais|obitos_novos| ano|mes|          data_carga|\n",
      "+----------+------+--------------------+------------+-----------+-------------+------------+----+---+--------------------+\n",
      "|2023-03-18|    SE|          Aracaju/SE|      169414|         32|         2616|           0|2023|  3|2025-10-15 20:57:...|\n",
      "|2023-03-18|    PA|           Bujaru/PA|        2081|          4|           43|           0|2023|  3|2025-10-15 20:57:...|\n",
      "|2023-03-18|    MT|CASO SEM LOCALIZA...|           0|          0|            0|           0|2023|  3|2025-10-15 20:57:...|\n",
      "|2023-03-18|    RS|    Cacique Doble/RS|        1443|          0|           15|           0|2023|  3|2025-10-15 20:57:...|\n",
      "|2023-03-18|    RS|    Caxias do Sul/RS|      168401|        255|         1701|           0|2023|  3|2025-10-15 20:57:...|\n",
      "|2023-03-18|    SP|Cristais Paulista/SP|        1307|          0|           20|           0|2023|  3|2025-10-15 20:57:...|\n",
      "|2023-03-18|    SP|           Fern√£o/SP|         747|          0|            5|           0|2023|  3|2025-10-15 20:57:...|\n",
      "|2023-03-18|    MG|       Fervedouro/MG|        1655|          0|           20|           0|2023|  3|2025-10-15 20:57:...|\n",
      "|2023-03-18|    BA|            Ipir√°/BA|        3329|          1|           80|           0|2023|  3|2025-10-15 20:57:...|\n",
      "|2023-03-18|    PE|         Itapetim/PE|        2879|          1|           39|           0|2023|  3|2025-10-15 20:57:...|\n",
      "+----------+------+--------------------+------------+-----------+-------------+------------+----+---+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "üìä Estat√≠sticas dos dados limpos:\n",
      "+-------+-----------------+-----------------+------------------+------------------+\n",
      "|summary|     casos_totais|      casos_novos|     obitos_totais|      obitos_novos|\n",
      "+-------+-----------------+-----------------+------------------+------------------+\n",
      "|  count|             5596|             5596|              5596|              5596|\n",
      "|   mean|6627.147962830593|              0.0|124.96604717655468|               0.0|\n",
      "| stddev|33093.52967699999|352.5162297330447| 916.3480862158888|2.1629754768178606|\n",
      "|    min|           -37026|           -17239|              -258|              -121|\n",
      "|    max|          1316242|            10164|             44671|                52|\n",
      "+-------+-----------------+-----------------+------------------+------------------+\n",
      "\n",
      "\n",
      "=== AN√ÅLISES B√ÅSICAS ===\n",
      "\n",
      "1. Top 10 Estados com mais casos:\n",
      "+------+-----------+------------+\n",
      "|estado|total_casos|total_obitos|\n",
      "+------+-----------+------------+\n",
      "|    SP|    6469442|      179039|\n",
      "|    MG|    4189760|       65507|\n",
      "|    RS|    2962414|       41921|\n",
      "|    PR|    2920177|       46022|\n",
      "|    RJ|    2754359|       76852|\n",
      "|    SC|    1986447|       22703|\n",
      "|    GO|    1900459|       28041|\n",
      "|    BA|    1792572|       31459|\n",
      "|    CE|    1453417|       28157|\n",
      "|    ES|    1323595|       15041|\n",
      "+------+-----------+------------+\n",
      "\n",
      "\n",
      "2. Casos por ano:\n",
      "+----+---------+----------+\n",
      "| ano|casos_ano|obitos_ano|\n",
      "+----+---------+----------+\n",
      "|2023|        0|         0|\n",
      "+----+---------+----------+\n",
      "\n",
      "\n",
      "3. Dados mais recentes (√∫ltimas 5 datas):\n",
      "+----------+------+--------------------+------------+-----------+-------------+------------+----+---+--------------------+\n",
      "|      data|estado|              cidade|casos_totais|casos_novos|obitos_totais|obitos_novos| ano|mes|          data_carga|\n",
      "+----------+------+--------------------+------------+-----------+-------------+------------+----+---+--------------------+\n",
      "|2023-03-18|    SE|          Aracaju/SE|      169414|         32|         2616|           0|2023|  3|2025-10-15 20:57:...|\n",
      "|2023-03-18|    PA|           Bujaru/PA|        2081|          4|           43|           0|2023|  3|2025-10-15 20:57:...|\n",
      "|2023-03-18|    MT|CASO SEM LOCALIZA...|           0|          0|            0|           0|2023|  3|2025-10-15 20:57:...|\n",
      "|2023-03-18|    RS|    Cacique Doble/RS|        1443|          0|           15|           0|2023|  3|2025-10-15 20:57:...|\n",
      "|2023-03-18|    RS|    Caxias do Sul/RS|      168401|        255|         1701|           0|2023|  3|2025-10-15 20:57:...|\n",
      "+----------+------+--------------------+------------+-----------+-------------+------------+----+---+--------------------+\n",
      "\n",
      "\n",
      "=== SALVANDO DADOS LIMPOS ===\n",
      "üíæ Salvando como CSV...\n",
      "‚úì CSV salvo: ../data/processed/covid19_brazil_clean.csv\n",
      "‚úì Registros salvos: 5,596\n",
      "\n",
      "üíæ Salvando como Parquet...\n",
      "‚úì Parquet salvo: ../data/processed/covid19_brazil_clean.parquet\n",
      "\n",
      "=== FASE 3: LOAD (Carregamento) ===\n",
      "üìã Preparando dados para SQL Server...\n",
      "\n",
      "üîΩ Criando amostra para SQL Server...\n",
      "‚úì Amostra criada: 1,000 registros\n",
      "\n",
      "üìä Primeiras linhas da amostra:\n",
      "+----------+------+--------------------+------------+-----------+-------------+------------+----+---+--------------------+\n",
      "|      data|estado|              cidade|casos_totais|casos_novos|obitos_totais|obitos_novos| ano|mes|          data_carga|\n",
      "+----------+------+--------------------+------------+-----------+-------------+------------+----+---+--------------------+\n",
      "|2023-03-18|    SE|          Aracaju/SE|      169414|         32|         2616|           0|2023|  3|2025-10-15 20:57:...|\n",
      "|2023-03-18|    PA|           Bujaru/PA|        2081|          4|           43|           0|2023|  3|2025-10-15 20:57:...|\n",
      "|2023-03-18|    MT|CASO SEM LOCALIZA...|           0|          0|            0|           0|2023|  3|2025-10-15 20:57:...|\n",
      "|2023-03-18|    RS|    Cacique Doble/RS|        1443|          0|           15|           0|2023|  3|2025-10-15 20:57:...|\n",
      "|2023-03-18|    RS|    Caxias do Sul/RS|      168401|        255|         1701|           0|2023|  3|2025-10-15 20:57:...|\n",
      "+----------+------+--------------------+------------+-----------+-------------+------------+----+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "=== GERANDO SCRIPT SQL ===\n",
      "‚úì Script SQL salvo: ../sql/create_table_covid19.sql\n",
      "\n",
      "üìù Conte√∫do do script:\n",
      "\n",
      "-- ============================================\n",
      "-- Script: create_table_covid19.sql\n",
      "-- Gerado automaticamente pelo pipeline ETL\n",
      "-- Data: 16/10/2025\n",
      "-- ============================================\n",
      "\n",
      "USE datalake_local;\n",
      "GO\n",
      "\n",
      "-- Limpar tabela se existir\n",
      "IF OBJECT_ID('dbo.covid19_brazil', 'U') IS NOT NULL\n",
      "    DROP TABLE dbo.covid19_brazil;\n",
      "GO\n",
      "\n",
      "-- Criar tabela\n",
      "CREATE TABLE dbo.covid19_brazil (\n",
      "    id INT PRIMARY KEY IDENTITY(1,1),\n",
      "    data DATE NOT NULL,\n",
      "    estado VARCHAR(2),\n",
      "    cidade VARCHAR(200),\n",
      "    casos_totais INT,\n",
      "    casos_novos INT,\n",
      "    obitos_totais INT,\n",
      "    obitos_novos INT,\n",
      "    ano INT,\n",
      "    mes INT,\n",
      "    data_carga DATETIME DEFAULT GETDATE()\n",
      ");\n",
      "GO\n",
      "\n",
      "-- Criar √≠ndices para melhor performance\n",
      "CREATE INDEX idx_data ON dbo.covid19_brazil(data);\n",
      "CREATE INDEX idx_estado ON dbo.covid19_brazil(estado);\n",
      "CREATE INDEX idx_cidade ON dbo.covid19_brazil(cidade);\n",
      "GO\n",
      "\n",
      "PRINT 'Tabela covid19_brazil criada com sucesso!';\n",
      "GO\n",
      "\n",
      "\n",
      "=== GERANDO DADOS PARA INSER√á√ÉO ===\n",
      "üìù Gerando comandos INSERT...\n",
      "‚úì Script de INSERT salvo: ../sql/insert_covid19_sample.sql\n",
      "‚úì Total de INSERTs: 10\n",
      "\n",
      "üìù Exemplo de INSERT gerado:\n",
      "-- Inserir dados de exemplo\n",
      "USE datalake_local;\n",
      "GO\n",
      "\n",
      "INSERT INTO dbo.covid19_brazil (data, estado, cidade, casos_totais, casos_novos, obitos_totais, obitos_novos, ano, mes) VALUES ('2023-03-18', 'SE', 'Aracaju/SE', 169414, 32, 2616, 0, 2023, 3);\n",
      "INSERT INTO dbo.covid19_brazil (data, estado, cidade, casos_totais, casos_novos, obitos_totais, obitos_novos, ano, mes) VALUES ('2023-03-18', 'PA', 'Bujaru/PA', 2081, 4, 43, 0, 2023, 3);\n",
      "INSERT INTO dbo.covid19_brazil (data, estado, cidade, casos_totais, casos_novos, obitos_totais, obitos_novos, ano, mes) VALUES ('2023-03-18', 'MT', 'CASO SEM LOCALIZA√á√ÉO DEFINIDA/MT', 0, 0, 0, 0, 2023, 3);\n",
      "\n",
      "\n",
      "============================================================\n",
      "=== RESUMO DO PIPELINE ETL ===\n",
      "============================================================\n",
      "\n",
      "‚úÖ EXTRACT (Extra√ß√£o):\n",
      "   ‚Ä¢ Fonte: COVID-19 Brasil (GitHub)\n",
      "   ‚Ä¢ Arquivo baixado: ../data/raw/covid19_brazil_raw.csv\n",
      "   ‚Ä¢ Registros originais: 5,596\n",
      "\n",
      "‚úÖ TRANSFORM (Transforma√ß√£o):\n",
      "   ‚Ä¢ Colunas selecionadas: 10\n",
      "   ‚Ä¢ Registros ap√≥s limpeza: 5,596\n",
      "   ‚Ä¢ Transforma√ß√µes aplicadas:\n",
      "     - Remo√ß√£o de nulos\n",
      "     - Remo√ß√£o de duplicatas\n",
      "     - Convers√£o de tipos\n",
      "     - Colunas calculadas (ano, m√™s)\n",
      "     - Renomea√ß√£o para portugu√™s\n",
      "\n",
      "‚úÖ LOAD (Carregamento):\n",
      "   ‚Ä¢ CSV processado: ../data/processed/covid19_brazil_clean.csv\n",
      "   ‚Ä¢ Parquet gerado: ../data/processed/covid19_brazil_clean.parquet\n",
      "   ‚Ä¢ Script SQL: ../sql/create_table_covid19.sql\n",
      "   ‚Ä¢ Inserts SQL: ../sql/insert_covid19_sample.sql\n",
      "   ‚Ä¢ Amostra para SQL: 1,000 registros\n",
      "\n",
      "============================================================\n",
      "üéâ PIPELINE ETL COMPLETO!\n",
      "============================================================\n",
      "\n",
      "üìã PR√ìXIMOS PASSOS:\n",
      "1. Execute o script SQL no SSMS para criar a tabela\n",
      "2. Execute o script de INSERT para popular com dados de exemplo\n",
      "3. Ou use uma ferramenta de ETL para carregar todos os dados\n",
      "\n",
      "=== FINALIZANDO ===\n",
      "‚úì Pipeline conclu√≠do com sucesso!\n",
      "‚úì Spark Session mantida ativa para an√°lises adicionais\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Notebook: etl_covid_to_sql.ipynb\n",
    "# Objetivo: Pipeline ETL - CSV ‚Üí DataFrame ‚Üí SQL Server\n",
    "# Data: 16/10/2025\n",
    "# Dataset: COVID-19 Brasil (dados p√∫blicos)\n",
    "# ============================================\n",
    "\n",
    "# C√âLULA 1: Configurar ambiente\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configura√ß√µes do Spark\n",
    "python_path = sys.executable\n",
    "os.environ['PYSPARK_PYTHON'] = python_path\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = python_path\n",
    "os.environ['SPARK_LOCAL_HOSTNAME'] = 'localhost'\n",
    "\n",
    "print(\"=== CONFIGURA√á√ÉO DO AMBIENTE ===\")\n",
    "print(f\"‚úì Python: {sys.version.split()[0]}\")\n",
    "print(f\"‚úì Execut√°vel: {python_path}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Importar bibliotecas\n",
    "print(\"\\n=== IMPORTANDO BIBLIOTECAS ===\")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "print(\"‚úì Bibliotecas importadas!\")\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# C√âLULA 2: Criar Spark Session\n",
    "print(\"\\n=== CRIANDO SPARK SESSION ===\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL_COVID_to_SQL\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"‚úì Spark Session criada!\")\n",
    "print(f\"‚úì Vers√£o: {spark.version}\")\n",
    "print(f\"‚úì App Name: {spark.sparkContext.appName}\")\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# C√âLULA 3: EXTRACT - Baixar dados p√∫blicos de COVID-19\n",
    "print(\"\\n=== FASE 1: EXTRACT (Extra√ß√£o) ===\")\n",
    "\n",
    "# URL do dataset p√∫blico (COVID-19 Brasil - atualizado diariamente)\n",
    "url = \"https://raw.githubusercontent.com/wcota/covid19br/master/cases-brazil-cities.csv\"\n",
    "\n",
    "# Criar diret√≥rio para dados brutos\n",
    "os.makedirs(\"../data/raw\", exist_ok=True)\n",
    "raw_file = \"../data/raw/covid19_brazil_raw.csv\"\n",
    "\n",
    "print(f\"üì• Baixando dados de: {url}\")\n",
    "try:\n",
    "    urllib.request.urlretrieve(url, raw_file)\n",
    "    print(f\"‚úì Dados baixados: {raw_file}\")\n",
    "    \n",
    "    # Verificar tamanho do arquivo\n",
    "    file_size = os.path.getsize(raw_file) / (1024 * 1024)  # MB\n",
    "    print(f\"‚úì Tamanho: {file_size:.2f} MB\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao baixar: {e}\")\n",
    "    print(\"‚ö†Ô∏è Usando arquivo de exemplo local...\")\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# C√âLULA 4: Ler CSV com Spark\n",
    "print(\"\\n=== LENDO CSV COM SPARK ===\")\n",
    "\n",
    "# Ler CSV\n",
    "df_raw = spark.read.csv(\n",
    "    raw_file,\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    sep=\",\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì CSV lido com sucesso!\")\n",
    "print(f\"‚úì Total de registros: {df_raw.count():,}\")\n",
    "print(f\"‚úì Total de colunas: {len(df_raw.columns)}\")\n",
    "\n",
    "# Mostrar primeiras linhas\n",
    "print(\"\\nüìä Primeiras linhas dos dados brutos:\")\n",
    "df_raw.show(5, truncate=False)\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# C√âLULA 5: Explorar dados brutos\n",
    "print(\"\\n=== AN√ÅLISE EXPLORAT√ìRIA ===\")\n",
    "\n",
    "# Schema\n",
    "print(\"\\n1. Schema dos dados:\")\n",
    "df_raw.printSchema()\n",
    "\n",
    "# Colunas\n",
    "print(\"\\n2. Colunas dispon√≠veis:\")\n",
    "for i, col in enumerate(df_raw.columns, 1):\n",
    "    print(f\"   {i}. {col}\")\n",
    "\n",
    "# Estat√≠sticas b√°sicas\n",
    "print(\"\\n3. Estat√≠sticas descritivas:\")\n",
    "df_raw.describe().show()\n",
    "\n",
    "# Valores nulos\n",
    "print(\"\\n4. Contagem de valores nulos por coluna:\")\n",
    "from pyspark.sql.functions import col, sum as spark_sum, count, when\n",
    "\n",
    "null_counts = df_raw.select([\n",
    "    spark_sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "    for c in df_raw.columns\n",
    "])\n",
    "null_counts.show()\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# C√âLULA 6: TRANSFORM - Limpeza e Transforma√ß√£o\n",
    "print(\"\\n=== FASE 2: TRANSFORM (Transforma√ß√£o) ===\")\n",
    "\n",
    "print(\"üßπ Iniciando limpeza dos dados...\")\n",
    "\n",
    "# 1. Selecionar apenas colunas relevantes\n",
    "colunas_interesse = [\n",
    "    'date',           # Data\n",
    "    'state',          # Estado\n",
    "    'city',           # Cidade\n",
    "    'totalCases',     # Total de casos\n",
    "    'newCases',       # Novos casos\n",
    "    'deaths',         # √ìbitos totais\n",
    "    'newDeaths'       # Novos √≥bitos\n",
    "]\n",
    "\n",
    "# Verificar se as colunas existem\n",
    "colunas_disponiveis = [c for c in colunas_interesse if c in df_raw.columns]\n",
    "print(f\"‚úì Colunas selecionadas: {colunas_disponiveis}\")\n",
    "\n",
    "df_clean = df_raw.select(colunas_disponiveis)\n",
    "\n",
    "# 2. Remover linhas com valores nulos nas colunas cr√≠ticas\n",
    "print(\"\\nüîç Removendo valores nulos...\")\n",
    "registros_antes = df_clean.count()\n",
    "df_clean = df_clean.dropna(subset=['date', 'state', 'city'])\n",
    "registros_depois = df_clean.count()\n",
    "print(f\"‚úì Registros removidos: {registros_antes - registros_depois:,}\")\n",
    "print(f\"‚úì Registros restantes: {registros_depois:,}\")\n",
    "\n",
    "# 3. Remover duplicatas\n",
    "print(\"\\nüîç Removendo duplicatas...\")\n",
    "registros_antes = df_clean.count()\n",
    "df_clean = df_clean.dropDuplicates(['date', 'state', 'city'])\n",
    "registros_depois = df_clean.count()\n",
    "print(f\"‚úì Duplicatas removidas: {registros_antes - registros_depois:,}\")\n",
    "\n",
    "# 4. Converter tipos de dados\n",
    "print(\"\\nüîÑ Convertendo tipos de dados...\")\n",
    "df_clean = df_clean.withColumn('date', to_date(col('date'), 'yyyy-MM-dd'))\n",
    "df_clean = df_clean.withColumn('totalCases', col('totalCases').cast('integer'))\n",
    "df_clean = df_clean.withColumn('newCases', col('newCases').cast('integer'))\n",
    "df_clean = df_clean.withColumn('deaths', col('deaths').cast('integer'))\n",
    "df_clean = df_clean.withColumn('newDeaths', col('newDeaths').cast('integer'))\n",
    "\n",
    "# 5. Adicionar colunas calculadas\n",
    "print(\"\\n‚ûï Adicionando colunas calculadas...\")\n",
    "df_clean = df_clean.withColumn('year', year(col('date')))\n",
    "df_clean = df_clean.withColumn('month', month(col('date')))\n",
    "df_clean = df_clean.withColumn('data_carga', current_timestamp())\n",
    "\n",
    "# 6. Renomear colunas para portugu√™s\n",
    "print(\"\\nüî§ Renomeando colunas...\")\n",
    "df_clean = df_clean \\\n",
    "    .withColumnRenamed('date', 'data') \\\n",
    "    .withColumnRenamed('state', 'estado') \\\n",
    "    .withColumnRenamed('city', 'cidade') \\\n",
    "    .withColumnRenamed('totalCases', 'casos_totais') \\\n",
    "    .withColumnRenamed('newCases', 'casos_novos') \\\n",
    "    .withColumnRenamed('deaths', 'obitos_totais') \\\n",
    "    .withColumnRenamed('newDeaths', 'obitos_novos') \\\n",
    "    .withColumnRenamed('year', 'ano') \\\n",
    "    .withColumnRenamed('month', 'mes')\n",
    "\n",
    "print(\"‚úì Transforma√ß√£o conclu√≠da!\")\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# C√âLULA 7: Visualizar dados limpos\n",
    "print(\"\\n=== DADOS LIMPOS ===\")\n",
    "\n",
    "print(\"\\nüìä Schema final:\")\n",
    "df_clean.printSchema()\n",
    "\n",
    "print(\"\\nüìä Amostra dos dados limpos:\")\n",
    "df_clean.show(10)\n",
    "\n",
    "print(\"\\nüìä Estat√≠sticas dos dados limpos:\")\n",
    "df_clean.select('casos_totais', 'casos_novos', 'obitos_totais', 'obitos_novos').describe().show()\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# C√âLULA 8: An√°lises b√°sicas\n",
    "print(\"\\n=== AN√ÅLISES B√ÅSICAS ===\")\n",
    "\n",
    "# 1. Total por estado\n",
    "print(\"\\n1. Top 10 Estados com mais casos:\")\n",
    "df_clean.groupBy('estado') \\\n",
    "    .agg(\n",
    "        sum('casos_totais').alias('total_casos'),\n",
    "        sum('obitos_totais').alias('total_obitos')\n",
    "    ) \\\n",
    "    .orderBy(col('total_casos').desc()) \\\n",
    "    .limit(10) \\\n",
    "    .show()\n",
    "\n",
    "# 2. Evolu√ß√£o temporal\n",
    "print(\"\\n2. Casos por ano:\")\n",
    "df_clean.groupBy('ano') \\\n",
    "    .agg(\n",
    "        sum('casos_novos').alias('casos_ano'),\n",
    "        sum('obitos_novos').alias('obitos_ano')\n",
    "    ) \\\n",
    "    .orderBy('ano') \\\n",
    "    .show()\n",
    "\n",
    "# 3. Dados mais recentes\n",
    "print(\"\\n3. Dados mais recentes (√∫ltimas 5 datas):\")\n",
    "df_clean.orderBy(col('data').desc()).limit(5).show()\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# C√âLULA 9: Salvar dados limpos\n",
    "print(\"\\n=== SALVANDO DADOS LIMPOS ===\")\n",
    "\n",
    "# Criar diret√≥rio para dados processados\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "# Salvar como CSV (usando Pandas para evitar problema do Hadoop)\n",
    "print(\"üíæ Salvando como CSV...\")\n",
    "processed_file = \"../data/processed/covid19_brazil_clean.csv\"\n",
    "df_clean_pandas = df_clean.toPandas()\n",
    "df_clean_pandas.to_csv(processed_file, index=False)\n",
    "print(f\"‚úì CSV salvo: {processed_file}\")\n",
    "print(f\"‚úì Registros salvos: {len(df_clean_pandas):,}\")\n",
    "\n",
    "# Salvar como Parquet (formato mais eficiente)\n",
    "print(\"\\nüíæ Salvando como Parquet...\")\n",
    "parquet_path = \"../data/processed/covid19_brazil_clean.parquet\"\n",
    "df_clean_pandas.to_parquet(parquet_path, index=False)\n",
    "print(f\"‚úì Parquet salvo: {parquet_path}\")\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# C√âLULA 10: LOAD - Preparar para SQL Server\n",
    "print(\"\\n=== FASE 3: LOAD (Carregamento) ===\")\n",
    "\n",
    "print(\"üìã Preparando dados para SQL Server...\")\n",
    "\n",
    "# Para o SQL Server, vamos criar uma amostra menor (√∫ltimos 1000 registros)\n",
    "print(\"\\nüîΩ Criando amostra para SQL Server...\")\n",
    "df_sample = df_clean.orderBy(col('data').desc()).limit(1000)\n",
    "\n",
    "print(f\"‚úì Amostra criada: {df_sample.count():,} registros\")\n",
    "print(\"\\nüìä Primeiras linhas da amostra:\")\n",
    "df_sample.show(5)\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# C√âLULA 11: Criar script SQL para tabela\n",
    "print(\"\\n=== GERANDO SCRIPT SQL ===\")\n",
    "\n",
    "sql_script = \"\"\"\n",
    "-- ============================================\n",
    "-- Script: create_table_covid19.sql\n",
    "-- Gerado automaticamente pelo pipeline ETL\n",
    "-- Data: 16/10/2025\n",
    "-- ============================================\n",
    "\n",
    "USE datalake_local;\n",
    "GO\n",
    "\n",
    "-- Limpar tabela se existir\n",
    "IF OBJECT_ID('dbo.covid19_brazil', 'U') IS NOT NULL\n",
    "    DROP TABLE dbo.covid19_brazil;\n",
    "GO\n",
    "\n",
    "-- Criar tabela\n",
    "CREATE TABLE dbo.covid19_brazil (\n",
    "    id INT PRIMARY KEY IDENTITY(1,1),\n",
    "    data DATE NOT NULL,\n",
    "    estado VARCHAR(2),\n",
    "    cidade VARCHAR(200),\n",
    "    casos_totais INT,\n",
    "    casos_novos INT,\n",
    "    obitos_totais INT,\n",
    "    obitos_novos INT,\n",
    "    ano INT,\n",
    "    mes INT,\n",
    "    data_carga DATETIME DEFAULT GETDATE()\n",
    ");\n",
    "GO\n",
    "\n",
    "-- Criar √≠ndices para melhor performance\n",
    "CREATE INDEX idx_data ON dbo.covid19_brazil(data);\n",
    "CREATE INDEX idx_estado ON dbo.covid19_brazil(estado);\n",
    "CREATE INDEX idx_cidade ON dbo.covid19_brazil(cidade);\n",
    "GO\n",
    "\n",
    "PRINT 'Tabela covid19_brazil criada com sucesso!';\n",
    "GO\n",
    "\"\"\"\n",
    "\n",
    "# Salvar script SQL\n",
    "sql_file = \"../sql/create_table_covid19.sql\"\n",
    "os.makedirs(\"../sql\", exist_ok=True)\n",
    "with open(sql_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(sql_script)\n",
    "\n",
    "print(f\"‚úì Script SQL salvo: {sql_file}\")\n",
    "print(\"\\nüìù Conte√∫do do script:\")\n",
    "print(sql_script)\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# C√âLULA 12: Gerar INSERT SQL\n",
    "print(\"\\n=== GERANDO DADOS PARA INSER√á√ÉO ===\")\n",
    "\n",
    "# Converter amostra para formato de INSERT\n",
    "print(\"üìù Gerando comandos INSERT...\")\n",
    "\n",
    "# Pegar apenas alguns registros para exemplo\n",
    "df_insert = df_sample.limit(10).toPandas()\n",
    "\n",
    "insert_statements = []\n",
    "insert_statements.append(\"-- Inserir dados de exemplo\\n\")\n",
    "insert_statements.append(\"USE datalake_local;\\nGO\\n\\n\")\n",
    "\n",
    "for _, row in df_insert.iterrows():\n",
    "    insert = f\"\"\"INSERT INTO dbo.covid19_brazil (data, estado, cidade, casos_totais, casos_novos, obitos_totais, obitos_novos, ano, mes) VALUES ('{row['data']}', '{row['estado']}', '{row['cidade']}', {row['casos_totais']}, {row['casos_novos']}, {row['obitos_totais']}, {row['obitos_novos']}, {row['ano']}, {row['mes']});\"\"\"\n",
    "    insert_statements.append(insert + \"\\n\")\n",
    "\n",
    "insert_statements.append(\"\\nGO\\n\")\n",
    "insert_statements.append(\"\\nPRINT 'Dados inseridos com sucesso!';\\nGO\")\n",
    "\n",
    "# Salvar inserts\n",
    "insert_file = \"../sql/insert_covid19_sample.sql\"\n",
    "with open(insert_file, 'w', encoding='utf-8') as f:\n",
    "    f.writelines(insert_statements)\n",
    "\n",
    "print(f\"‚úì Script de INSERT salvo: {insert_file}\")\n",
    "print(f\"‚úì Total de INSERTs: {len(df_insert)}\")\n",
    "\n",
    "# Mostrar exemplo\n",
    "print(\"\\nüìù Exemplo de INSERT gerado:\")\n",
    "print(''.join(insert_statements[:5]))\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# C√âLULA 13: Resumo do Pipeline ETL\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== RESUMO DO PIPELINE ETL ===\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n‚úÖ EXTRACT (Extra√ß√£o):\")\n",
    "print(f\"   ‚Ä¢ Fonte: COVID-19 Brasil (GitHub)\")\n",
    "print(f\"   ‚Ä¢ Arquivo baixado: {raw_file}\")\n",
    "print(f\"   ‚Ä¢ Registros originais: {df_raw.count():,}\")\n",
    "\n",
    "print(\"\\n‚úÖ TRANSFORM (Transforma√ß√£o):\")\n",
    "print(f\"   ‚Ä¢ Colunas selecionadas: {len(df_clean.columns)}\")\n",
    "print(f\"   ‚Ä¢ Registros ap√≥s limpeza: {df_clean.count():,}\")\n",
    "print(f\"   ‚Ä¢ Transforma√ß√µes aplicadas:\")\n",
    "print(f\"     - Remo√ß√£o de nulos\")\n",
    "print(f\"     - Remo√ß√£o de duplicatas\")\n",
    "print(f\"     - Convers√£o de tipos\")\n",
    "print(f\"     - Colunas calculadas (ano, m√™s)\")\n",
    "print(f\"     - Renomea√ß√£o para portugu√™s\")\n",
    "\n",
    "print(\"\\n‚úÖ LOAD (Carregamento):\")\n",
    "print(f\"   ‚Ä¢ CSV processado: {processed_file}\")\n",
    "print(f\"   ‚Ä¢ Parquet gerado: {parquet_path}\")\n",
    "print(f\"   ‚Ä¢ Script SQL: {sql_file}\")\n",
    "print(f\"   ‚Ä¢ Inserts SQL: {insert_file}\")\n",
    "print(f\"   ‚Ä¢ Amostra para SQL: {df_sample.count():,} registros\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ PIPELINE ETL COMPLETO!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìã PR√ìXIMOS PASSOS:\")\n",
    "print(\"1. Execute o script SQL no SSMS para criar a tabela\")\n",
    "print(\"2. Execute o script de INSERT para popular com dados de exemplo\")\n",
    "print(\"3. Ou use uma ferramenta de ETL para carregar todos os dados\")\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# C√âLULA 14: Parar Spark Session\n",
    "print(\"\\n=== FINALIZANDO ===\")\n",
    "# spark.stop()  # Descomente para encerrar\n",
    "print(\"‚úì Pipeline conclu√≠do com sucesso!\")\n",
    "print(\"‚úì Spark Session mantida ativa para an√°lises adicionais\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd5c982-9c79-41a7-aa18-65629d401d58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
